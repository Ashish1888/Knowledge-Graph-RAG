ğŸ“š K-Graph RAG â€” Hybrid Knowledge Graph + Vector RAG (FastAPI + Groq Llama 3.3)

A fully working K-Graph RAG (Knowledge-Graph-Augmented Retrieval-Augmented Generation) system built using:

ğŸ§  Knowledge Graph (NetworkX)

ğŸ“¦ Vector Store (FAISS / numpy fallback)

ğŸ” Chunk-based semantic retrieval

ğŸ”— Triple extraction via LLM

ğŸ— Graph-informed multi-hop reasoning

ğŸ¤– Groq LLaMA 3.3-70B for entity extraction + generation

âš¡ FastAPI backend, testable using Postman

This project stores all data locally in:

data/meta.json     # vector metadata for chunks
data/graph.json    # KG triples as JSON
data/faiss.index   # vector embeddings (if FAISS available)
data/emb.npy       # fallback embeddings (numpy)


âœ” This is a clean, modular, production-style implementation of a K-Graph RAG pipeline.

â­ Features
ğŸ”µ Knowledge Graph (KG)

Auto-extracted triples like:

Alice -works_at-> Microsoft
Microsoft -headquartered_in-> Redmond


Multi-hop graph traversal (hops=1/2/3)

ğŸŸ¢ Vector Store

SentenceTransformers embeddings

FAISS (fast) or numpy (fallback)

Stores all chunks + document metadata in JSON

ğŸŸ£ LLM Logic (via Groq)

Triple extraction

Entity extraction

Final answer generation using KG + vector context

ğŸ”¥ API Endpoints

POST /ingest â€” ingest documents, chunk them, embed them, extract triples

POST /query â€” hybrid KG + vector retrieval + optional generation

GET /health â€” check totals and system status

ğŸŸ© Pure backend â€” fully testable with Postman, cURL, or any frontend.
ğŸ“‚ Project Structure
/project-root
â”‚
â”œâ”€â”€ main.py                # FastAPI app (routes)
â”œâ”€â”€ vector_store.py        # Vector storage + FAISS index
â”œâ”€â”€ graph_store.py         # Knowledge graph storage
â”œâ”€â”€ utils.py               # Groq client, triple/entity extraction, chunking
â”‚
â”œâ”€â”€ data/                  # Auto-created
â”‚   â”œâ”€â”€ meta.json          # JSON metadata for chunks
â”‚   â”œâ”€â”€ graph.json         # JSON KG triples
â”‚   â”œâ”€â”€ faiss.index        # Vector index (FAISS)
â”‚   â””â”€â”€ emb.npy            # Embedding matrix (fallback)
â”‚
â”œâ”€â”€ .env                   # API keys + config
â””â”€â”€ README.md              # Documentation

âš™ï¸ Installation
1. Clone the repository
git clone https://github.com/your-repo/kgraph-rag.git
cd kgraph-rag

2. Create a virtual environment
python -m venv venv
source venv/bin/activate   # mac / linux
venv\Scripts\activate      # windows

3. Install dependencies
pip install -r requirements.txt

4. Create .env

Create a .env file in the project root:

GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile

DATA_DIR=data
EMBED_MODEL=all-MiniLM-L6-v2

REQUIRE_API_KEY=false
API_KEY_HEADER=supersecret

ğŸš€ Run the Server
uvicorn main:app --reload


FastAPI will start at:

http://localhost:8000


Interactive docs:

http://localhost:8000/docs

ğŸ“¥ Ingest Documents (POST /ingest)
Example Request (Postman / cURL)
{
  "documents": [
    {
      "id": "doc1",
      "text": "Alice works at Microsoft. Microsoft is headquartered in Redmond."
    },
    {
      "id": "doc2",
      "text": "Bob works at Google. Google is based in Mountain View."
    }
  ]
}

ğŸ” Query (POST /query)
{
  "question": "Where does Alice work and where is that company located?",
  "top_k": 5,
  "hops": 2,
  "use_generation": true
}

Response Example
{
    "question": "Where does Alice work and where is that company located?",
    "entities": ["Alice", "Microsoft"],
    "graph": [
        {"sub":"Alice","rel":"works_at","obj":"Microsoft"},
        {"sub":"Microsoft","rel":"headquartered_in","obj":"Redmond"}
    ],
    "retrieved": [
        {
            "score": 0.60,
            "doc_id": "doc1",
            "chunk_id": "doc1::chunk::0",
            "text": "Alice works at Microsoft..."
        }
    ],
    "answer": "Alice works at Microsoft and Microsoft is headquartered in Redmond."
}

ğŸ§  How K-Graph RAG Works
âœ” Step 1 â€” Ingestion

Split text into chunks

Generate embeddings â†’ store in meta.json

Extract triples â†’ store in graph.json

âœ” Step 2 â€” Query

Extract entities from question (LLM)

Traverse knowledge graph (1â€“3 hops)

Retrieve top-k vector chunks

Combine KG + retrieved text â†’ final answer

This gives better reasoning than vector-only RAG.