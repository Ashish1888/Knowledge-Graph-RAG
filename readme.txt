ğŸ“š K-Graph RAG â€” Hybrid Knowledge Graph + Vector RAG (FastAPI + Groq LLaMA 3.3)

A fully working K-Graph RAG (Knowledge-Graph-Augmented Retrieval-Augmented Generation) system that combines graph reasoning and semantic vector search.
Built with:

ğŸ§  Knowledge Graph (NetworkX)

ğŸ“¦ Vector Store (FAISS / numpy fallback)

ğŸ” Chunk-based semantic retrieval

ğŸ”— Triple extraction via LLM (Groq LLaMA 3.3)

ğŸ— Graph-informed multi-hop reasoning

âš¡ FastAPI backend

ğŸ“¬ Fully testable using Postman / cURL

ğŸ“‚ Local Storage Files
data/meta.json     # vector metadata for chunks
data/graph.json    # KG triples as JSON
data/faiss.index   # vector embeddings (if FAISS installed)
data/emb.npy       # fallback embeddings using numpy


âœ” The system is modular, fully local, and production-style.

â­ Features
ğŸ”µ Knowledge Graph (KG)

Extracts triples automatically:

Alice -works_at-> Microsoft
Microsoft -headquartered_in-> Redmond


Supports multi-hop reasoning (hops=1/2/3)

ğŸŸ¢ Vector Store

Uses SentenceTransformers for embeddings

FAISS for high-speed vector search

JSON-based metadata for easy debugging

ğŸŸ£ LLM Logic (Groq)

Triple extraction

Entity extraction

LLM answer generation using KG facts + retrieved chunks

ğŸ”¥ API Endpoints

POST /ingest â†’ document ingestion, chunking, embedding, triple extraction

POST /query â†’ hybrid retrieval + optional answer generation

GET /health â†’ system + data stats

ğŸ“ Project Structure
project-root/
â”‚
â”œâ”€â”€ main.py                # FastAPI app (routes)
â”œâ”€â”€ vector_store.py        # Vector storage + FAISS index
â”œâ”€â”€ graph_store.py         # Knowledge graph storage
â”œâ”€â”€ utils.py               # Groq client, triple/entity extraction, chunking
â”‚
â”œâ”€â”€ data/                  # Auto-created at runtime
â”‚   â”œâ”€â”€ meta.json          # JSON metadata for chunks
â”‚   â”œâ”€â”€ graph.json         # JSON KG triples
â”‚   â”œâ”€â”€ faiss.index        # FAISS vector index
â”‚   â””â”€â”€ emb.npy            # Fallback embedding matrix
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env                   # API keys + config (never push to GitHub)
â””â”€â”€ README.md

âš™ï¸ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/your-repo/kgraph-rag.git
cd kgraph-rag

2ï¸âƒ£ Create and activate a virtual environment
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ”§ Create .env

Make a .env file in the project root:

GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile

DATA_DIR=data
EMBED_MODEL=all-MiniLM-L6-v2

REQUIRE_API_KEY=false
API_KEY_HEADER=supersecret


âš  .env is included in .gitignore.

ğŸš€ Run the Server
uvicorn main:app --reload


FastAPI runs at:

API Base: http://localhost:8000

Docs: http://localhost:8000/docs

ğŸ“¥ Ingest Documents (POST /ingest)
Example Request
{
  "documents": [
    {
      "id": "doc1",
      "text": "Alice works at Microsoft. Microsoft is headquartered in Redmond."
    },
    {
      "id": "doc2",
      "text": "Bob works at Google. Google is based in Mountain View."
    }
  ]
}

What ingestion does:

Chunks text

Creates embeddings â†’ stores in vector DB

Extracts triples â†’ stores in graph.json

ğŸ” Query (POST /query)
Example Request
{
  "question": "Where does Alice work and where is that company located?",
  "top_k": 5,
  "hops": 2,
  "use_generation": true
}

Example Response
{
  "question": "Where does Alice work and where is that company located?",
  "entities": ["Alice", "Microsoft"],
  "graph": [
    {"sub": "Alice", "rel": "works_at", "obj": "Microsoft"},
    {"sub": "Microsoft", "rel": "headquartered_in", "obj": "Redmond"}
  ],
  "retrieved": [
    {
      "score": 0.60,
      "doc_id": "doc1",
      "chunk_id": "doc1::chunk::0",
      "text": "Alice works at Microsoft..."
    }
  ],
  "answer": "Alice works at Microsoft and Microsoft is headquartered in Redmond."
}

ğŸ§  How K-Graph RAG Works
âœ” Step 1 â€” Ingestion

Split document into chunks

Generate embeddings

Store embeddings in FAISS / emb.npy

Extract triples (LLM)

Append triples to graph.json

âœ” Step 2 â€” Query

Extract entities from question (LLM)

Traverse knowledge graph (1â€“3 hops)

Retrieve top-k vector chunks

Build final answer from KG facts + chunk retrieval

This combination provides more accuracy and explainability than vector-only RAG.
